{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NVidia GPU Architecture (and comparisons)\n",
    "\n",
    "Picture of the NVidia Ampere from nvidia.com\n",
    "\n",
    "<img src=\"https://www.nvidia.com/content/dam/en-zz/Solutions/gtcf21/ampere-architecture/dpu-a100x_front-2c50-d.jpg\" width=512 />\n",
    "\n",
    "### CUDA Architecture Evolution\n",
    "\n",
    "Ampere <- Turing <- Pascal <- Maxwell <- Kepler <- Fermi <- Tesla <- G70\n",
    "  * G70 is GeForce Series 7 2005\n",
    "\n",
    "#### Turing Architecture\n",
    "\n",
    "<img src=\"https://devblogs.nvidia.com/wp-content/uploads/2018/09/image2.jpg\" width=768 />\n",
    "\n",
    "* 4,608 CUDA Cores\n",
    "  * 64 cores / SM\n",
    "* 672 GB/sec memory throughput\n",
    "* 6 MB L2 memory\n",
    "\n",
    "<img src=\"https://devblogs.nvidia.com/wp-content/uploads/2018/09/image11.jpg\" width=512 />\n",
    "\n",
    "Properties:\n",
    "  * 16.3 GFlops of double precision floating point\n",
    "  * 32.6 GFlops of single precisions floating point\n",
    "  * 16.3 TIPs of integer arithmetic\n",
    "  * Reduced precision tensor cores (INT8 and INT4)\n",
    "    * for ML workloads that can tolerate quantization\n",
    "    \n",
    "What's different than a CPU?\n",
    "  * High core count\n",
    "  * Different use of transistor \"real-estate\".  Roughly\n",
    "    * CPU -- ~50% of transistors in managed caches, ~20% in I/O\n",
    "    * GPU -- ~80% of transistors in FP/IP\n",
    "  * Very small SM private L1 cache. \n",
    "  * Small shared L2 cache.\n",
    "    \n",
    "CPU image of AMD Ryzen 5000 from [https://wccftech.com/amd-ryzen-5000-zen-3-vermeer-undressed-high-res-die-shots-close-ups-pictured-detailed/](https://wccftech.com/amd-ryzen-5000-zen-3-vermeer-undressed-high-res-die-shots-close-ups-pictured-detailed/)\n",
    "\n",
    "<img src=\"https://cdn.wccftech.com/wp-content/uploads/2020/11/AMD-Ryzen-5000-Zen-3-Desktop-CPU_Vermeer_Die-Shot_1-2048x1350.jpg\" width=512 />\n",
    "\n",
    "\n",
    "### Properties of CUDA (all architectures)\n",
    "\n",
    "* Fundamental unit is the CUDA core\n",
    "  * Integer arithmetic logic unit ALU \n",
    "  * Double-precision floating point FPU\n",
    "  * Fused multiply-add instruction Fully pipelined\n",
    "* CUDA cores are grouped into stream multiprocessors (SM)\n",
    "  * Each SM runs in SIMD lockstep, i.e. is a vector processor\n",
    "* Observations\n",
    "  * High memory throughput (now 672 GB/s) c.f. 256 GB/S on Intel Skylake\n",
    "  * Little cache -- essentially useless\n",
    "\n",
    "### Programming Model\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/5/59/CUDA_processing_flow_%28En%29.PNG\" width=512 />\n",
    "\n",
    "* Transfer data to accelerator\n",
    "  * limited by PCIe speed, typically 8GB/s one way\n",
    "* Invoke remote computation\n",
    "* Extract result\n",
    "\n",
    "#### Consequences\n",
    "* For CUDA to be effective, the computation must be intense w.r.t. the data.\n",
    "  * 8 GB/s of transfer compared with 600 GB/s memory to processor\n",
    "  * _Must_ use data values multiple time\n",
    "* CUDA programs must be simple\n",
    "  * FP and integer arithmetic\n",
    "  * no cache hierarchy to support data reuse\n",
    "  * similarly no HW support for branching and speculation (more on this later).\n",
    "  \n",
    "Connecting back to __Roofline performance__.\n",
    "  * Requires kernels with high intensity\n",
    "  * CUDA has fused multiply add -- this a form of ILP\n",
    "  * There are two off chip transfers:\n",
    "    * CPU memory -> GPU memory\n",
    "    * GPU memory -> SMs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recent Developments\n",
    "\n",
    "The changes from the origianl (2005) GPU arcehitecture that have made CUDA general purpose.\n",
    "\n",
    "* Double precision\n",
    "    * graphics cards were always single precision.  Why?\n",
    "* Memory system\n",
    "    * L1 cache per SM\n",
    "    * Global L2 cache\n",
    "    * ECC (error correcting memory)\n",
    "        * why does graphics not need ECC memory?\n",
    "* Concurrent kernel execution\n",
    "    * kernel is the name for a CUDA program\n",
    "    * used to be one kernel at a time\n",
    "* Muliple-GPU interconnects\n",
    "    * fast data transfer among GPUs\n",
    "    * needed from ML training\n",
    "\n",
    "<img src=\"https://en.wikichip.org/w/images/8/88/nvidia_dgx-1_nvlink-gpu-xeon_config.svg\" width=512 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So What??\n",
    "\n",
    "CUDA is delivering FLOPs faster, cheaper and at less power than CPUs.\n",
    "\n",
    "<img src=\"https://www.karlrupp.net/wp-content/uploads/2013/06/gflops-sp.png\" width=512 />\n",
    "\n",
    "GPUs are everywhere:\n",
    "\n",
    "  * on supercomputers as accelerators\n",
    "  * on laptops already because they have screens\n",
    "  * on phones (or architectures inspired by GPU principles) because of power\n",
    "  \n",
    "### System on a Chip\n",
    "\n",
    "System on a Chip architectures have graphics (GPU) built in. These architectures will dominate desktop and laptop market going forward.\n",
    "\n",
    "  * Apple M1 (with shared GPU/CPU memory. no copying.)\n",
    "  * Intel Alder Lake (shown below)\n",
    "  * AMD Exynos with Samsung coming later in 2021\n",
    "\n",
    "<img src=\"https://preview.redd.it/lyhmdzo6c3w71.jpg?width=960&crop=smart&auto=webp&s=89fa3a1cf9d1925bb5f4922333e9c902e7c75148\" width=768 />\n",
    "\n",
    "So what about GPU PCIe cards???? \n",
    "* In supercomputers\n",
    "* On the cloud\n",
    "  * machine learning training\n",
    "  * collaborative/cloud gaming\n",
    "  * rendering, video editing \n",
    "  * ????\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
